{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# æ·±åº¦å­¦ä¹ æ—¶é—´åºåˆ—é¢„æµ‹\n\n**LSTM ä¸ Transformer åœ¨èµ„é‡‘è´¹ç‡é¢„æµ‹ä¸­çš„åº”ç”¨**\n\n---\n\n## ç›®å½•\n\n### ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€çŸ¥è¯†\n1. æ—¶é—´åºåˆ—é¢„æµ‹ç®€ä»‹\n2. æ·±åº¦å­¦ä¹ åœ¨é‡‘èé¢„æµ‹ä¸­çš„åº”ç”¨\n3. LSTM ä¸ Transformer æ¶æ„æ¦‚è¿°\n\n### ç¬¬äºŒéƒ¨åˆ†ï¼šLSTM æ¨¡å‹\n4. LSTM åŸç†è¯¦è§£\n5. æ•°æ®å‡†å¤‡ä¸ç‰¹å¾å·¥ç¨‹\n6. LSTM æ¨¡å‹æ„å»ºï¼ˆPyTorchï¼‰\n7. æ¨¡å‹è®­ç»ƒä¸éªŒè¯\n8. èµ„é‡‘è´¹ç‡é¢„æµ‹å®æˆ˜\n\n### ç¬¬ä¸‰éƒ¨åˆ†ï¼šTransformer æ¨¡å‹\n9. Transformer æ¶æ„è¯¦è§£\n10. æ—¶é—´åºåˆ— Transformer å®ç°\n11. Transformer èµ„é‡‘è´¹ç‡é¢„æµ‹\n\n### ç¬¬å››éƒ¨åˆ†ï¼šæ¨¡å‹å¯¹æ¯”ä¸ä¼˜åŒ–\n12. LSTM vs Transformer æ€§èƒ½å¯¹æ¯”\n13. è¶…å‚æ•°è°ƒä¼˜\n14. å®æˆ˜å»ºè®®ä¸æ€»ç»“"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["---\n\n# ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€çŸ¥è¯†\n\n---"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 1. æ—¶é—´åºåˆ—é¢„æµ‹ç®€ä»‹\n\n### 1.1 ä»€ä¹ˆæ˜¯æ—¶é—´åºåˆ—é¢„æµ‹ï¼Ÿ\n\n**æ—¶é—´åºåˆ—é¢„æµ‹** æ˜¯åˆ©ç”¨å†å²æ•°æ®æ¥é¢„æµ‹æœªæ¥å€¼çš„æ–¹æ³•ã€‚åœ¨é‡‘èé¢†åŸŸï¼Œå®ƒè¢«å¹¿æ³›ç”¨äºï¼š\n- ä»·æ ¼é¢„æµ‹\n- æ³¢åŠ¨ç‡é¢„æµ‹\n- èµ„é‡‘è´¹ç‡é¢„æµ‹\n- äº¤æ˜“é‡é¢„æµ‹\n\n### 1.2 æ—¶é—´åºåˆ—çš„ç‰¹ç‚¹\n\n| ç‰¹æ€§ | æè¿° | èµ„é‡‘è´¹ç‡ç¤ºä¾‹ |\n|------|------|-------------|\n| **è¶‹åŠ¿ï¼ˆTrendï¼‰** | é•¿æœŸä¸Šå‡æˆ–ä¸‹é™è¶‹åŠ¿ | ç‰›å¸‚æœŸé—´è´¹ç‡æ•´ä½“åé«˜ |\n| **å­£èŠ‚æ€§ï¼ˆSeasonalityï¼‰** | å‘¨æœŸæ€§é‡å¤æ¨¡å¼ | ç»“ç®—æ—¶é—´ç‚¹çš„è§„å¾‹æ€§ |\n| **å‘¨æœŸæ€§ï¼ˆCyclicalityï¼‰** | éå›ºå®šå‘¨æœŸçš„æ³¢åŠ¨ | å¸‚åœºæƒ…ç»ªå‘¨æœŸ |\n| **éšæœºæ€§ï¼ˆNoiseï¼‰** | æ— æ³•è§£é‡Šçš„æ³¢åŠ¨ | çªå‘äº‹ä»¶å½±å“ |\n\n### 1.3 ä¼ ç»Ÿæ–¹æ³• vs æ·±åº¦å­¦ä¹ \n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                      é¢„æµ‹æ–¹æ³•å¯¹æ¯”                               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚   ä¼ ç»Ÿç»Ÿè®¡æ–¹æ³•                    æ·±åº¦å­¦ä¹ æ–¹æ³•                  â”‚\nâ”‚   â”œâ”€â”€ ARIMA                      â”œâ”€â”€ RNN/LSTM                  â”‚\nâ”‚   â”œâ”€â”€ GARCH                      â”œâ”€â”€ GRU                       â”‚\nâ”‚   â”œâ”€â”€ Prophet                    â”œâ”€â”€ Transformer               â”‚\nâ”‚   â””â”€â”€ VAR                        â””â”€â”€ TCN                       â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 2. æ·±åº¦å­¦ä¹ åœ¨é‡‘èé¢„æµ‹ä¸­çš„åº”ç”¨\n\n### 2.1 ä¸ºä»€ä¹ˆé€‰æ‹©æ·±åº¦å­¦ä¹ ï¼Ÿ\n\né‡‘èæ—¶é—´åºåˆ—å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š\n1. **éçº¿æ€§å…³ç³»**ï¼šä»·æ ¼å˜åŠ¨ä¸å½±å“å› ç´ ä¹‹é—´å­˜åœ¨å¤æ‚çš„éçº¿æ€§å…³ç³»\n2. **é•¿æœŸä¾èµ–**ï¼šå½“å‰ä»·æ ¼å¯èƒ½å—åˆ°å¾ˆä¹…ä¹‹å‰äº‹ä»¶çš„å½±å“\n3. **å¤šå˜é‡äº¤äº’**ï¼šå¤šä¸ªå› ç´ ç›¸äº’å½±å“\n4. **å™ªå£°å¹²æ‰°**ï¼šä¿¡å·ä¸å™ªå£°æ··åˆ"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3. LSTM ä¸ Transformer æ¶æ„æ¦‚è¿°\n\n### 3.1 LSTMï¼ˆLong Short-Term Memoryï¼‰\n\nLSTM æ˜¯ä¸€ç§ç‰¹æ®Šçš„ RNNï¼Œé€šè¿‡é—¨æ§æœºåˆ¶è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼š\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        LSTM å•å…ƒç»“æ„                            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚   ä¸‰ä¸ªé—¨çš„ä½œç”¨ï¼š                                               â”‚\nâ”‚   â€¢ é—å¿˜é—¨ï¼šå†³å®šä¸¢å¼ƒå“ªäº›æ—§ä¿¡æ¯                                 â”‚\nâ”‚   â€¢ è¾“å…¥é—¨ï¼šå†³å®šå­˜å‚¨å“ªäº›æ–°ä¿¡æ¯                                 â”‚\nâ”‚   â€¢ è¾“å‡ºé—¨ï¼šå†³å®šè¾“å‡ºå“ªäº›ä¿¡æ¯                                   â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### 3.2 Transformer\n\nTransformer ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯ä»¥å¹¶è¡Œå¤„ç†åºåˆ—ã€‚\n\n### 3.3 LSTM vs Transformer å¯¹æ¯”\n\n| ç‰¹æ€§ | LSTM | Transformer |\n|------|------|-------------|\n| **å¤„ç†æ–¹å¼** | é¡ºåºå¤„ç† | å¹¶è¡Œå¤„ç† |\n| **é•¿æœŸä¾èµ–** | é€šè¿‡é—¨æ§æœºåˆ¶ | é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ |\n| **è®¡ç®—å¤æ‚åº¦** | O(n) | O(nÂ²) |\n| **è®­ç»ƒé€Ÿåº¦** | è¾ƒæ…¢ | è¾ƒå¿« |"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["---\n\n# ç¬¬äºŒéƒ¨åˆ†ï¼šLSTM æ¨¡å‹\n\n---"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 4. LSTM åŸç†è¯¦è§£\n\n### 4.1 LSTM æ•°å­¦å…¬å¼\n\n**é—å¿˜é—¨ï¼ˆForget Gateï¼‰**ï¼š\n$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n\n**è¾“å…¥é—¨ï¼ˆInput Gateï¼‰**ï¼š\n$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n\n**ç»†èƒçŠ¶æ€æ›´æ–°**ï¼š\n$$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$\n\n**è¾“å‡ºé—¨ï¼ˆOutput Gateï¼‰**ï¼š\n$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n$$h_t = o_t \\odot \\tanh(C_t)$$"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 5. æ•°æ®å‡†å¤‡ä¸ç‰¹å¾å·¥ç¨‹"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# 5.1 å¯¼å…¥ä¾èµ–åº“\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport ccxt\nfrom datetime import datetime, timedelta\nimport warnings\n\n# æ·±åº¦å­¦ä¹ åº“\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# æ£€æŸ¥ GPU å¯ç”¨æ€§\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'âœ… ä½¿ç”¨è®¾å¤‡: {device}')\nprint(f'âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# 5.2 è·å–èµ„é‡‘è´¹ç‡æ•°æ®\n\ndef get_funding_rate_history(symbol: str, days: int = 180) -> pd.DataFrame:\n    \"\"\"\n    è·å–å†å²èµ„é‡‘è´¹ç‡\n    \"\"\"\n    exchange = ccxt.binance({\n        'enableRateLimit': True,\n        'options': {'defaultType': 'swap'}\n    })\n    \n    since = int((datetime.now() - timedelta(days=days)).timestamp() * 1000)\n    \n    all_rates = []\n    while True:\n        rates = exchange.fetch_funding_rate_history(symbol, since=since, limit=1000)\n        if not rates:\n            break\n        all_rates.extend(rates)\n        since = rates[-1]['timestamp'] + 1\n        if len(rates) < 1000:\n            break\n    \n    df = pd.DataFrame(all_rates)\n    if not df.empty:\n        df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')\n        df = df.set_index('datetime')\n    \n    return df\n\n# è·å– BTC èµ„é‡‘è´¹ç‡æ•°æ®\nprint('ğŸ“Š è·å– BTC èµ„é‡‘è´¹ç‡æ•°æ®...')\ndf_funding = get_funding_rate_history('BTC/USDT:USDT', days=365)\nprint(f'âœ… è·å–åˆ° {len(df_funding)} æ¡è®°å½•')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# 5.3 æ•°æ®é¢„å¤„ç†ä¸ç‰¹å¾å·¥ç¨‹\n\ndef create_features(df: pd.DataFrame, target_col: str = 'fundingRate') -> pd.DataFrame:\n    \"\"\"\n    åˆ›å»ºæ—¶é—´åºåˆ—ç‰¹å¾\n    \"\"\"\n    df_feat = df[[target_col]].copy()\n    \n    # æ»åç‰¹å¾\n    for i in range(1, 25):\n        df_feat[f'lag_{i}'] = df_feat[target_col].shift(i)\n    \n    # æ»šåŠ¨ç»Ÿè®¡\n    for window in [3, 6, 12, 24]:\n        df_feat[f'rolling_mean_{window}'] = df_feat[target_col].shift(1).rolling(window).mean()\n        df_feat[f'rolling_std_{window}'] = df_feat[target_col].shift(1).rolling(window).std()\n    \n    # æ—¶é—´ç‰¹å¾\n    df_feat['hour'] = df_feat.index.hour / 24\n    df_feat['day_of_week'] = df_feat.index.dayofweek / 7\n    \n    return df_feat.dropna()\n\ndf_features = create_features(df_funding)\nprint(f'âœ… åˆ›å»ºç‰¹å¾å®Œæˆï¼Œå…± {len(df_features.columns)} åˆ—')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# 5.4 åˆ›å»ºåºåˆ—æ•°æ®é›†\n\nclass FundingRateDataset(Dataset):\n    def __init__(self, data: np.ndarray, seq_length: int = 24):\n        self.data = data\n        self.seq_length = seq_length\n    \n    def __len__(self):\n        return len(self.data) - self.seq_length\n    \n    def __getitem__(self, idx):\n        x = self.data[idx:idx + self.seq_length]\n        y = self.data[idx + self.seq_length, 0]\n        return torch.FloatTensor(x), torch.FloatTensor([y])\n\n\ndef prepare_data(df: pd.DataFrame, seq_length: int = 24, train_ratio: float = 0.8):\n    scaler = StandardScaler()\n    data_scaled = scaler.fit_transform(df.values)\n    \n    train_size = int(len(data_scaled) * train_ratio)\n    train_data = data_scaled[:train_size]\n    test_data = data_scaled[train_size:]\n    \n    train_dataset = FundingRateDataset(train_data, seq_length)\n    test_dataset = FundingRateDataset(test_data, seq_length)\n    \n    return train_dataset, test_dataset, scaler\n\nSEQ_LENGTH = 24\nBATCH_SIZE = 32\n\ntrain_dataset, test_dataset, scaler = prepare_data(df_features, SEQ_LENGTH)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nprint(f'âœ… æ•°æ®å‡†å¤‡å®Œæˆ')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 6. LSTM æ¨¡å‹æ„å»º"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# 6.1 LSTM æ¨¡å‹å®šä¹‰\n\nclass LSTMPredictor(nn.Module):\n    def __init__(self, input_dim: int, hidden_dim: int = 64, num_layers: int = 2, \n                 output_dim: int = 1, dropout: float = 0.2):\n        super(LSTMPredictor, self).__init__()\n        \n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        \n        self.lstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n        \n        self.fc = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Linear(hidden_dim // 2, output_dim)\n        )\n    \n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        out = lstm_out[:, -1, :]\n        out = self.fc(out)\n        return out\n\nINPUT_DIM = df_features.shape[1]\nlstm_model = LSTMPredictor(input_dim=INPUT_DIM, hidden_dim=64, num_layers=2).to(device)\nprint(lstm_model)\nprint(f'\\nâœ… æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in lstm_model.parameters()):,}')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 7. æ¨¡å‹è®­ç»ƒä¸éªŒè¯"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# 7.1 è®­ç»ƒå‡½æ•°\n\ndef train_model(model, train_loader, test_loader, epochs=100, lr=0.001, patience=10):\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n    \n    train_losses, test_losses = [], []\n    best_loss = float('inf')\n    patience_counter = 0\n    \n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            optimizer.zero_grad()\n            y_pred = model(X_batch)\n            loss = criterion(y_pred, y_batch)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            train_loss += loss.item()\n        \n        train_loss /= len(train_loader)\n        train_losses.append(train_loss)\n        \n        model.eval()\n        test_loss = 0\n        with torch.no_grad():\n            for X_batch, y_batch in test_loader:\n                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n                y_pred = model(X_batch)\n                test_loss += criterion(y_pred, y_batch).item()\n        \n        test_loss /= len(test_loader)\n        test_losses.append(test_loss)\n        scheduler.step(test_loss)\n        \n        if test_loss < best_loss:\n            best_loss = test_loss\n            patience_counter = 0\n            best_model_state = model.state_dict().copy()\n        else:\n            patience_counter += 1\n        \n        if (epoch + 1) % 10 == 0:\n            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}')\n        \n        if patience_counter >= patience:\n            print(f'æ—©åœäº Epoch {epoch+1}')\n            model.load_state_dict(best_model_state)\n            break\n    \n    return train_losses, test_losses"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# 7.2 è®­ç»ƒ LSTM æ¨¡å‹\n\nprint('ğŸ“Š å¼€å§‹è®­ç»ƒ LSTM æ¨¡å‹...')\ntrain_losses, test_losses = train_model(lstm_model, train_loader, test_loader, epochs=100, lr=0.001, patience=15)\nprint('âœ… è®­ç»ƒå®Œæˆï¼')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# 7.3 è®­ç»ƒæ›²çº¿å¯è§†åŒ–\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(y=train_losses, mode='lines', name='è®­ç»ƒæŸå¤±', line=dict(color='blue')))\nfig.add_trace(go.Scatter(y=test_losses, mode='lines', name='æµ‹è¯•æŸå¤±', line=dict(color='red')))\nfig.update_layout(title='LSTM è®­ç»ƒæ›²çº¿', xaxis_title='Epoch', yaxis_title='Loss', height=400)\nfig.show()"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 8. èµ„é‡‘è´¹ç‡é¢„æµ‹å®æˆ˜"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# 8.1 æ¨¡å‹è¯„ä¼°\n\ndef evaluate_model(model, test_loader, scaler):\n    model.eval()\n    predictions, actuals = [], []\n    \n    with torch.no_grad():\n        for X_batch, y_batch in test_loader:\n            X_batch = X_batch.to(device)\n            y_pred = model(X_batch)\n            predictions.extend(y_pred.cpu().numpy())\n            actuals.extend(y_batch.numpy())\n    \n    predictions = np.array(predictions).flatten()\n    actuals = np.array(actuals).flatten()\n    \n    # åæ ‡å‡†åŒ–\n    scale, mean = scaler.scale_[0], scaler.mean_[0]\n    predictions = predictions * scale + mean\n    actuals = actuals * scale + mean\n    \n    mae = mean_absolute_error(actuals, predictions)\n    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n    r2 = r2_score(actuals, predictions)\n    direction_correct = np.mean((predictions[1:] > predictions[:-1]) == (actuals[1:] > actuals[:-1]))\n    \n    return {'predictions': predictions, 'actuals': actuals, 'mae': mae, 'rmse': rmse, 'r2': r2, 'direction_accuracy': direction_correct}\n\nlstm_results = evaluate_model(lstm_model, test_loader, scaler)\nprint(f'MAE: {lstm_results[\"mae\"]*100:.6f}%')\nprint(f'RMSE: {lstm_results[\"rmse\"]*100:.6f}%')\nprint(f'RÂ²: {lstm_results[\"r2\"]:.4f}')\nprint(f'æ–¹å‘å‡†ç¡®ç‡: {lstm_results[\"direction_accuracy\"]*100:.2f}%')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["---\n\n# ç¬¬ä¸‰éƒ¨åˆ†ï¼šTransformer æ¨¡å‹\n\n---"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 9. Transformer æ¶æ„è¯¦è§£\n\n### 9.1 è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰\n\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\n### 9.2 å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰\n\n$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n\n### 9.3 ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰\n\n$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 10. æ—¶é—´åºåˆ— Transformer å®ç°"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# 10.1 ä½ç½®ç¼–ç \n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        \n        self.register_buffer('pe', pe)\n    \n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1), :]\n        return self.dropout(x)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# 10.2 Transformer é¢„æµ‹æ¨¡å‹\n\nclass TransformerPredictor(nn.Module):\n    def __init__(self, input_dim: int, d_model: int = 64, nhead: int = 4, num_layers: int = 2,\n                 dim_feedforward: int = 256, dropout: float = 0.1, output_dim: int = 1):\n        super(TransformerPredictor, self).__init__()\n        \n        self.input_embedding = nn.Linear(input_dim, d_model)\n        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,\n            dropout=dropout, batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        \n        self.fc_out = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model // 2, output_dim)\n        )\n        self.d_model = d_model\n    \n    def forward(self, x):\n        x = self.input_embedding(x) * np.sqrt(self.d_model)\n        x = self.pos_encoder(x)\n        x = self.transformer_encoder(x)\n        x = x[:, -1, :]\n        return self.fc_out(x)\n\ntransformer_model = TransformerPredictor(input_dim=INPUT_DIM, d_model=64, nhead=4, num_layers=2).to(device)\nprint(transformer_model)\nprint(f'\\nâœ… æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in transformer_model.parameters()):,}')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 11. Transformer èµ„é‡‘è´¹ç‡é¢„æµ‹"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# 11.1 è®­ç»ƒ Transformer æ¨¡å‹\n\nprint('ğŸ“Š å¼€å§‹è®­ç»ƒ Transformer æ¨¡å‹...')\ntransformer_train_losses, transformer_test_losses = train_model(\n    transformer_model, train_loader, test_loader, epochs=100, lr=0.0005, patience=15\n)\nprint('âœ… è®­ç»ƒå®Œæˆï¼')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# 11.2 è¯„ä¼° Transformer æ¨¡å‹\n\ntransformer_results = evaluate_model(transformer_model, test_loader, scaler)\nprint(f'MAE: {transformer_results[\"mae\"]*100:.6f}%')\nprint(f'RMSE: {transformer_results[\"rmse\"]*100:.6f}%')\nprint(f'RÂ²: {transformer_results[\"r2\"]:.4f}')\nprint(f'æ–¹å‘å‡†ç¡®ç‡: {transformer_results[\"direction_accuracy\"]*100:.2f}%')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["---\n\n# ç¬¬å››éƒ¨åˆ†ï¼šæ¨¡å‹å¯¹æ¯”ä¸ä¼˜åŒ–\n\n---"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 12. LSTM vs Transformer æ€§èƒ½å¯¹æ¯”"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# 12.1 æ¨¡å‹å¯¹æ¯”\n\nprint('=' * 60)\nprint('ğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”')\nprint('=' * 60)\nprint(f'{\"æŒ‡æ ‡\":<15} {\"LSTM\":<20} {\"Transformer\":<20}')\nprint('-' * 60)\nprint(f'{\"MAE (%)\":<15} {lstm_results[\"mae\"]*100:<20.6f} {transformer_results[\"mae\"]*100:<20.6f}')\nprint(f'{\"RMSE (%)\":<15} {lstm_results[\"rmse\"]*100:<20.6f} {transformer_results[\"rmse\"]*100:<20.6f}')\nprint(f'{\"RÂ²\":<15} {lstm_results[\"r2\"]:<20.4f} {transformer_results[\"r2\"]:<20.4f}')\nprint(f'{\"æ–¹å‘å‡†ç¡®ç‡ (%)\":<15} {lstm_results[\"direction_accuracy\"]*100:<20.2f} {transformer_results[\"direction_accuracy\"]*100:<20.2f}')\nprint('=' * 60)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# 12.2 é¢„æµ‹å¯¹æ¯”å¯è§†åŒ–\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(y=lstm_results['actuals']*100, mode='lines', name='å®é™…å€¼', line=dict(color='blue', width=2)))\nfig.add_trace(go.Scatter(y=lstm_results['predictions']*100, mode='lines', name='LSTM', line=dict(color='red', dash='dash')))\nfig.add_trace(go.Scatter(y=transformer_results['predictions']*100, mode='lines', name='Transformer', line=dict(color='green', dash='dot')))\nfig.update_layout(title='LSTM vs Transformer é¢„æµ‹å¯¹æ¯”', xaxis_title='æ—¶é—´æ­¥', yaxis_title='èµ„é‡‘è´¹ç‡ (%)', height=500)\nfig.show()"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 13. è¶…å‚æ•°è°ƒä¼˜\n\n### å¸¸ç”¨è¶…å‚æ•°åŠå»ºè®®èŒƒå›´\n\n| è¶…å‚æ•° | LSTM å»ºè®®å€¼ | Transformer å»ºè®®å€¼ |\n|--------|-------------|-------------------|\n| hidden_dim/d_model | 32-128 | 32-128 |\n| num_layers | 1-3 | 2-4 |\n| dropout | 0.1-0.3 | 0.1-0.2 |\n| learning_rate | 1e-3 - 1e-4 | 5e-4 - 1e-4 |\n| batch_size | 16-64 | 16-64 |\n| seq_length | 12-48 | 24-96 |"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 14. å®æˆ˜å»ºè®®ä¸æ€»ç»“\n\n### 14.1 æ¨¡å‹é€‰æ‹©å»ºè®®\n\n| åœºæ™¯ | æ¨èæ¨¡å‹ | åŸå›  |\n|------|----------|------|\n| çŸ­æœŸé¢„æµ‹ | LSTM | æ›´ç¨³å®šï¼Œæˆæœ¬ä½ |\n| é•¿æœŸé¢„æµ‹ | Transformer | æ›´å¥½æ•æ‰é•¿æœŸä¾èµ– |\n| å¤šå˜é‡é¢„æµ‹ | Transformer | æ³¨æ„åŠ›æœºåˆ¶å¤„ç†å¤šç»´å…³ç³» |\n| èµ„æºå—é™ | GRU/LSTM | æ›´ä½çš„è®¡ç®—æˆæœ¬ |\n\n### 14.2 å¸¸è§é™·é˜±\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                      å¸¸è§é™·é˜±ä¸è§£å†³æ–¹æ¡ˆ                         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  1. è¿‡æ‹Ÿåˆ â†’ Dropoutã€æ­£åˆ™åŒ–ã€æ—©åœ                             â”‚\nâ”‚  2. æ•°æ®æ³„éœ² â†’ ä¸¥æ ¼çš„æ—¶é—´åˆ†å‰²                                  â”‚\nâ”‚  3. åˆ†å¸ƒæ¼‚ç§» â†’ å®šæœŸé‡è®­ç»ƒ                                      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# 14.3 é¢„æµ‹ä¸‹ä¸€æœŸ\n\ndef predict_next(model, df_features, scaler, seq_length):\n    model.eval()\n    recent_data = df_features.iloc[-seq_length:].values\n    recent_scaled = scaler.transform(recent_data)\n    \n    with torch.no_grad():\n        x = torch.FloatTensor(recent_scaled).unsqueeze(0).to(device)\n        pred = model(x).cpu().numpy()[0, 0]\n    \n    return pred * scaler.scale_[0] + scaler.mean_[0]\n\nlstm_pred = predict_next(lstm_model, df_features, scaler, SEQ_LENGTH)\ntransformer_pred = predict_next(transformer_model, df_features, scaler, SEQ_LENGTH)\n\nprint('ğŸ”® ä¸‹ä¸€æœŸèµ„é‡‘è´¹ç‡é¢„æµ‹')\nprint(f'LSTM: {lstm_pred*100:.4f}%  (å¹´åŒ–: {lstm_pred*3*365*100:.2f}%)')\nprint(f'Transformer: {transformer_pred*100:.4f}%  (å¹´åŒ–: {transformer_pred*3*365*100:.2f}%)')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["---\n\n## æ€»ç»“\n\n1. **LSTM**: é—¨æ§æœºåˆ¶è§£å†³é•¿æœŸä¾èµ–ï¼Œé€‚åˆä¸­çŸ­æœŸé¢„æµ‹\n2. **Transformer**: è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶è¡Œè®¡ç®—æ•ˆç‡é«˜ï¼Œé€‚åˆé•¿åºåˆ—\n3. **å®æˆ˜è¦ç‚¹**: ç‰¹å¾å·¥ç¨‹ã€é¿å…è¿‡æ‹Ÿåˆã€æ¨¡å‹é€‰æ‹©ä¸è°ƒä¼˜\n\n### ä¸‹ä¸€æ­¥å­¦ä¹ \n- [ ] å¤šäº¤æ˜“æ‰€å¥—åˆ©ç­–ç•¥\n- [ ] é«˜é¢‘äº¤æ˜“ä¸è®¢å•ç°¿åˆ†æ\n- [ ] æœŸæƒä¸æ°¸ç»­åˆçº¦ç»„åˆç­–ç•¥"]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.9.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
